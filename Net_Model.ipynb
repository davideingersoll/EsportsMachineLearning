{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<hr><hr><br>"
      ],
      "metadata": {
        "id": "HPT_t5TjI5IZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to <strong>Gerald</strong>\n",
        "<p>Gerald is a machine learning model that predict RL matchups based on past results. He doesn't dive into stats. He doesn't look at positioning. He doesn't look at goals, assists, boost, or any other in-game details. Gerald just looks at which players win and how different players interact on teams together and against other teams.</p><br>\n",
        "<p><strong>If you want to use Gerald without looking at the coding, skip to the next <i>text (not code)</i> section for instructions. </strong></p><br>\n",
        "<p>Everything before the next segments is model-training/model-building code that I have left in for those that want to look through. You can skip quickly using the table of contents</p>\n",
        "<br><hr><hr>"
      ],
      "metadata": {
        "id": "VomCAbd1n6Sh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Creation Section (skip for predictions)"
      ],
      "metadata": {
        "id": "qIiuv90IT0rv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Collection"
      ],
      "metadata": {
        "id": "yjOihjUo9rNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTndxscGP_-Y",
        "outputId": "a86f0b1e-af07-41a6-f4a5-ed9046d7f8b3",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cpu)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.45)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.6.0\n"
          ]
        }
      ],
      "source": [
        "%pip install torchmetrics\n",
        "%pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ForI09GlTc8b",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264ea41e-d67d-4ae2-f846-e1db208a1321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import requests\n",
        "import re\n",
        "import random\n",
        "import pickle\n",
        "import json\n",
        "import torch\n",
        "import math\n",
        "import random\n",
        "import optuna\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as f\n",
        "import torch.nn.init as init\n",
        "\n",
        "from collections import deque, Counter\n",
        "from datetime import datetime, timedelta\n",
        "from scipy import stats\n",
        "from torch import nn\n",
        "from torchmetrics import Accuracy\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from pathlib import Path\n",
        "from google.colab import userdata\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIpSLVHOzS8e"
      },
      "outputs": [],
      "source": [
        "Ballchasing_Token = userdata.get(\"BALLCHASING_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQxInkfmzZrw"
      },
      "outputs": [],
      "source": [
        "class RateLimiter:\n",
        "    #Token bucket rate limiter with dual constraints\n",
        "    def __init__(self, requests_per_second=2, requests_per_hour=1000):\n",
        "        self.rps = requests_per_second\n",
        "        self.rph = requests_per_hour\n",
        "        self.second_window = deque()\n",
        "        self.hour_window = deque()\n",
        "\n",
        "    def wait_if_needed(self):\n",
        "        now = time.time()\n",
        "\n",
        "        # Clean old entries\n",
        "        cutoff_second = now - 1.0\n",
        "        cutoff_hour = now - 3600.0\n",
        "\n",
        "        while self.second_window and self.second_window[0] < cutoff_second:\n",
        "            self.second_window.popleft()\n",
        "        while self.hour_window and self.hour_window[0] < cutoff_hour:\n",
        "            self.hour_window.popleft()\n",
        "\n",
        "        # Wait if at limits\n",
        "        if len(self.second_window) >= self.rps:\n",
        "            sleep_time = 1.0 - (now - self.second_window[0])\n",
        "            if sleep_time > 0:\n",
        "                time.sleep(sleep_time)\n",
        "\n",
        "        if len(self.hour_window) >= self.rph:\n",
        "            sleep_time = 3600.0 - (now - self.hour_window[0])\n",
        "            if sleep_time > 0:\n",
        "                print(f\"Hourly limit reached. Sleeping for {sleep_time/60:.1f} minutes...\")\n",
        "                time.sleep(sleep_time)\n",
        "\n",
        "        # Record this request\n",
        "        now = time.time()\n",
        "        self.second_window.append(now)\n",
        "        self.hour_window.append(now)\n",
        "\n",
        "\n",
        "class BallchasingCollector:\n",
        "    #Scalable data collector with checkpointing and error handling\n",
        "\n",
        "    def __init__(self, token, checkpoint_dir=\"/content/drive/MyDrive/checkpoints\"):\n",
        "        self.token = token\n",
        "        self.headers = {\n",
        "            'Authorization': token,\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        self.base_url = \"https://ballchasing.com/api\"\n",
        "        self.rate_limiter = RateLimiter()\n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Statistics\n",
        "        self.stats = {\n",
        "            'requests_made': 0,\n",
        "            'errors': 0,\n",
        "            'replays_processed': 0,\n",
        "            'start_time': None\n",
        "        }\n",
        "\n",
        "    def _make_request(self, url, max_retries=3):\n",
        "        #Make rate-limited request with exponential backoff\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                self.rate_limiter.wait_if_needed()\n",
        "                response = requests.get(url, headers=self.headers, timeout=30)\n",
        "                self.stats['requests_made'] += 1\n",
        "\n",
        "                if response.status_code == 429:  # Rate limited\n",
        "                    wait_time = min(2 ** attempt * 5, 60)\n",
        "                    print(f\"Rate limited. Waiting {wait_time}s...\")\n",
        "                    time.sleep(wait_time)\n",
        "                    continue\n",
        "\n",
        "                response.raise_for_status()\n",
        "                return response.json()\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                self.stats['errors'] += 1\n",
        "                print(f\"Error on attempt {attempt + 1}/{max_retries}: {e}\")\n",
        "                if attempt == max_retries - 1:\n",
        "                    raise\n",
        "                time.sleep(2 ** attempt)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def discover_replay_groups(self, group_ids, checkpoint_file=\"replay_groups.pkl\"):\n",
        "        #Discover all leaf groups containing replays using BFS\n",
        "        checkpoint_path = self.checkpoint_dir / checkpoint_file\n",
        "\n",
        "        # Try to load from checkpoint\n",
        "        if checkpoint_path.exists():\n",
        "            print(f\"Loading replay groups from checkpoint...\")\n",
        "            with open(checkpoint_path, 'rb') as f:\n",
        "                return pickle.load(f)\n",
        "\n",
        "        print(f\"Discovering replay groups from {len(group_ids)} root groups...\")\n",
        "        replay_groups = []\n",
        "        queue = deque(group_ids)\n",
        "        processed = set()\n",
        "\n",
        "        while queue:\n",
        "            group_id = queue.popleft()\n",
        "\n",
        "            if group_id in processed:\n",
        "                continue\n",
        "            processed.add(group_id)\n",
        "\n",
        "            print(f\"  Checking group: {group_id}\")\n",
        "\n",
        "            try:\n",
        "                data = self._make_request(f\"{self.base_url}/groups?group={group_id}\")\n",
        "\n",
        "                for sub_group in data.get('list', []):\n",
        "                    if sub_group['direct_replays'] > 0:\n",
        "                        replay_groups.append(sub_group['id'])\n",
        "                        print(f\"    ✓ Found {sub_group['direct_replays']} replays\")\n",
        "                    else:\n",
        "                        queue.append(sub_group['id'])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {group_id}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Save checkpoint\n",
        "        with open(checkpoint_path, 'wb') as f:\n",
        "            pickle.dump(replay_groups, f)\n",
        "\n",
        "        print(f\"Discovered {len(replay_groups)} groups with replays\")\n",
        "        return replay_groups\n",
        "\n",
        "    def get_replay_ids(self, replay_groups, checkpoint_file=\"replay_ids.pkl\"):\n",
        "        #Get all replay IDs from groups\n",
        "        checkpoint_path = self.checkpoint_dir / checkpoint_file\n",
        "\n",
        "        # Try to load from checkpoint\n",
        "        if checkpoint_path.exists():\n",
        "            print(f\"Loading replay IDs from checkpoint...\")\n",
        "            with open(checkpoint_path, 'rb') as f:\n",
        "                return pickle.load(f)\n",
        "\n",
        "        print(f\"Fetching replay IDs from {len(replay_groups)} groups...\")\n",
        "        replay_ids = []\n",
        "\n",
        "        for i, group_id in enumerate(replay_groups, 1):\n",
        "            try:\n",
        "                data = self._make_request(f\"{self.base_url}/replays?group={group_id}\")\n",
        "                group_replays = [replay['id'] for replay in data.get('list', [])]\n",
        "                replay_ids.extend(group_replays)\n",
        "                print(f\"  [{i}/{len(replay_groups)}] Group {group_id}: {len(group_replays)} replays\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Save checkpoint\n",
        "        with open(checkpoint_path, 'wb') as f:\n",
        "            pickle.dump(replay_ids, f)\n",
        "\n",
        "        print(f\"Found {len(replay_ids)} total replays\")\n",
        "        return replay_ids\n",
        "\n",
        "    def process_replays_streaming(self, replay_ids, batch_size=100, checkpoint_file=\"processed_data.csv\"):\n",
        "        #Process replays in batches and stream to CSV\n",
        "        checkpoint_path = self.checkpoint_dir / checkpoint_file\n",
        "\n",
        "        # Check what's already processed\n",
        "        processed_ids = set()\n",
        "        if checkpoint_path.exists():\n",
        "            existing_df = pd.read_csv(checkpoint_path)\n",
        "            if 'replay_id' in existing_df.columns:\n",
        "                processed_ids = set(existing_df['replay_id'].values)\n",
        "                print(f\"Resuming from checkpoint: {len(processed_ids)} already processed\")\n",
        "\n",
        "        remaining_ids = [rid for rid in replay_ids if rid not in processed_ids]\n",
        "        print(f\"Processing {len(remaining_ids)} replays ({len(processed_ids)} already done)\")\n",
        "\n",
        "        self.stats['start_time'] = datetime.now()\n",
        "\n",
        "        for i, replay_id in enumerate(remaining_ids, 1):\n",
        "            try:\n",
        "                data = self._make_request(f\"{self.base_url}/replays/{replay_id}\")\n",
        "                row = self._parse_replay(data, replay_id)\n",
        "\n",
        "                # Append to CSV\n",
        "                df = pd.DataFrame([row])\n",
        "                df.to_csv(checkpoint_path, mode='a', header=not checkpoint_path.exists(), index=False)\n",
        "\n",
        "                self.stats['replays_processed'] += 1\n",
        "\n",
        "                # Progress update\n",
        "                if i % 10 == 0:\n",
        "                    self._print_progress(i, len(remaining_ids))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing replay {replay_id}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"\\nProcessing complete!\")\n",
        "        self._print_stats()\n",
        "\n",
        "        return pd.read_csv(checkpoint_path)\n",
        "\n",
        "    def _parse_replay(self, data, replay_id):\n",
        "        # Parse replay JSON into flat dictionary\n",
        "        row = {'replay_id': replay_id}\n",
        "        row['created'] = data.get('created', None)\n",
        "\n",
        "        # Team info\n",
        "        row['team_name'] = data.get('blue', {}).get('name', 'Unknown')\n",
        "        row['opp_name'] = data.get('orange', {}).get('name', 'Unknown')\n",
        "\n",
        "        # Player names - pad to 3 players per team\n",
        "        blue_players = data.get('blue', {}).get('players', [])\n",
        "        orange_players = data.get('orange', {}).get('players', [])\n",
        "\n",
        "        # Pad blue team to 3 players\n",
        "        for i in range(1, 4):\n",
        "            if i <= len(blue_players):\n",
        "                row[f'team{i}'] = blue_players[i-1].get('name', f'Player{i}')\n",
        "            else:\n",
        "                row[f'team{i}'] = 'None'\n",
        "\n",
        "        # Pad orange team to 3 players\n",
        "        for i in range(1, 4):\n",
        "            if i <= len(orange_players):\n",
        "                row[f'opp{i}'] = orange_players[i-1].get('name', f'Player{i}')\n",
        "            else:\n",
        "                row[f'opp{i}'] = 'None'\n",
        "\n",
        "        # Calculate score\n",
        "        blue_goals = sum(p.get('stats', {}).get('core', {}).get('goals', 0) for p in blue_players)\n",
        "        orange_goals = sum(p.get('stats', {}).get('core', {}).get('goals', 0) for p in orange_players)\n",
        "\n",
        "        row['blue_goals'] = blue_goals\n",
        "        row['orange_goals'] = orange_goals\n",
        "        row['win'] = blue_goals > orange_goals\n",
        "\n",
        "        return row\n",
        "\n",
        "    def _print_progress(self, current, total):\n",
        "        #Print progress with ETA\n",
        "        elapsed = (datetime.now() - self.stats['start_time']).total_seconds()\n",
        "        rate = current / elapsed if elapsed > 0 else 0\n",
        "        remaining = total - current\n",
        "        eta_seconds = remaining / rate if rate > 0 else 0\n",
        "\n",
        "        print(f\"  [{current}/{total}] {current/total*100:.1f}% | \"\n",
        "              f\"Rate: {rate:.1f} replays/min | \"\n",
        "              f\"ETA: {eta_seconds/60:.1f} min\")\n",
        "\n",
        "    def _print_stats(self):\n",
        "        #Print collection statistics\n",
        "        elapsed = (datetime.now() - self.stats['start_time']).total_seconds()\n",
        "        print(f\"\\nStatistics:\")\n",
        "        print(f\"  Total requests: {self.stats['requests_made']}\")\n",
        "        print(f\"  Replays processed: {self.stats['replays_processed']}\")\n",
        "        print(f\"  Errors: {self.stats['errors']}\")\n",
        "        print(f\"  Time elapsed: {elapsed/60:.1f} minutes\")\n",
        "        print(f\"  Average rate: {self.stats['replays_processed']/elapsed*60:.1f} replays/min\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whUAlkxr44b9"
      },
      "outputs": [],
      "source": [
        "collector = BallchasingCollector(token=Ballchasing_Token, checkpoint_dir='/content/drive/MyDrive/Public RL/Complete ELO Model/checkpoints')\n",
        "groups = ['2-finals-211ggvb3eb']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2VNBU2Y5-F9",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "515b29d3-037e-4652-e33d-b2987c2f77f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading replay groups from checkpoint...\n"
          ]
        }
      ],
      "source": [
        "replay_groups = collector.discover_replay_groups(groups)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfcwuB-T6WH4",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e1ca35-aa02-40a4-9c9b-ce7e9a769a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading replay IDs from checkpoint...\n"
          ]
        }
      ],
      "source": [
        "replay_ids = collector.get_replay_ids(replay_groups)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIhQ0NW86Y_h",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eb8b7b6-5216-4fb1-a51e-d6f068400fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from checkpoint: 257 already processed\n",
            "Processing 0 replays (257 already done)\n",
            "\n",
            "Processing complete!\n",
            "\n",
            "Statistics:\n",
            "  Total requests: 0\n",
            "  Replays processed: 0\n",
            "  Errors: 0\n",
            "  Time elapsed: 0.0 minutes\n",
            "  Average rate: 0.0 replays/min\n"
          ]
        }
      ],
      "source": [
        "df = collector.process_replays_streaming(replay_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preparation and Model Construction"
      ],
      "metadata": {
        "id": "rXmGz4iA90zR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsTDqHfstmLb"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv('/content/drive/MyDrive/Public RL/Complete ELO Model/Legacy Data/Through_2026_Open1.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/Public RL/Complete ELO Model/Legacy Data/Fifae.csv')\n",
        "df = pd.concat([df1, df2], ignore_index=True)\n",
        "df['created'] = pd.to_datetime(df['created'])\n",
        "df = df.sort_values(by='created', ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59yTh6r2E1VT",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "c501ec81-8516-4512-e07c-214cd097c85d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     team_players                  opp_players  win\n",
              "0            [LCT, ballerrees, k]  [Dansku_, Porsas52, regser]    0\n",
              "1            [LCT, ballerrees, k]  [regser, Dansku_, Porsas52]    0\n",
              "2            [ballerrees, LCT, k]  [regser, Porsas52, Dansku_]    0\n",
              "3     [regser, Dansku_, Porsas52]      [loyal., Breezi, Kevin]    1\n",
              "4     [Dansku_, regser, Porsas52]      [Kevin, loyal., Breezi]    0\n",
              "...                           ...                          ...  ...\n",
              "7463     [Rw9, Kiileerrz, trk511]         [zen, juicy, vatira]    0\n",
              "7464     [trk511, Rw9, Kiileerrz]         [vatira, juicy, zen]    1\n",
              "7465     [trk511, Kiileerrz, Rw9]         [vatira, zen, juicy]    0\n",
              "7466     [Rw9, Kiileerrz, trk511]         [zen, vatira, juicy]    0\n",
              "7467     [Kiileerrz, Rw9, trk511]         [vatira, zen, juicy]    1\n",
              "\n",
              "[7468 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a7ec096-48c5-4f36-85ea-a5be6eb09284\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>team_players</th>\n",
              "      <th>opp_players</th>\n",
              "      <th>win</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[LCT, ballerrees, k]</td>\n",
              "      <td>[Dansku_, Porsas52, regser]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[LCT, ballerrees, k]</td>\n",
              "      <td>[regser, Dansku_, Porsas52]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[ballerrees, LCT, k]</td>\n",
              "      <td>[regser, Porsas52, Dansku_]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[regser, Dansku_, Porsas52]</td>\n",
              "      <td>[loyal., Breezi, Kevin]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Dansku_, regser, Porsas52]</td>\n",
              "      <td>[Kevin, loyal., Breezi]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7463</th>\n",
              "      <td>[Rw9, Kiileerrz, trk511]</td>\n",
              "      <td>[zen, juicy, vatira]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7464</th>\n",
              "      <td>[trk511, Rw9, Kiileerrz]</td>\n",
              "      <td>[vatira, juicy, zen]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7465</th>\n",
              "      <td>[trk511, Kiileerrz, Rw9]</td>\n",
              "      <td>[vatira, zen, juicy]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7466</th>\n",
              "      <td>[Rw9, Kiileerrz, trk511]</td>\n",
              "      <td>[zen, vatira, juicy]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7467</th>\n",
              "      <td>[Kiileerrz, Rw9, trk511]</td>\n",
              "      <td>[vatira, zen, juicy]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7468 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a7ec096-48c5-4f36-85ea-a5be6eb09284')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8a7ec096-48c5-4f36-85ea-a5be6eb09284 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8a7ec096-48c5-4f36-85ea-a5be6eb09284');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-be759c70-40e1-4167-af6e-6bc979f8a7c5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-be759c70-40e1-4167-af6e-6bc979f8a7c5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-be759c70-40e1-4167-af6e-6bc979f8a7c5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_b3ad7f36-3e43-4227-a4d5-85908e3bbd5a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b3ad7f36-3e43-4227-a4d5-85908e3bbd5a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 7468,\n  \"fields\": [\n    {\n      \"column\": \"team_players\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"opp_players\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"win\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 187
        }
      ],
      "source": [
        "#import dataframe and remove content exhibition matches\n",
        "df_3s = df.dropna()\n",
        "df_3s = df_3s[((df_3s['team_name'].str.lower() != 'first touch') & (df_3s['opp_name'].str.lower() != 'first_touch'))].reset_index(drop=True)\n",
        "\n",
        "#condense team/opp values to 2 lists of player names\n",
        "df_3s['team_players'] = df_3s[['team1','team2','team3']].apply(lambda row: row.to_list(), axis=1)\n",
        "df_3s['opp_players'] = df_3s[['opp1','opp2','opp3']].apply(lambda row: row.to_list(), axis=1)\n",
        "#recode win as numerical\n",
        "df_3s['win'] = df_3s['win'].map({True: 1, False:0})\n",
        "#sort dataframe for correct weighting later\n",
        "df_3s = df_3s.sort_values(by='created', ascending=True)\n",
        "#reduce data to relevant dimensions\n",
        "data = df_3s.copy()\n",
        "data = data[['team_players','opp_players','win']].reset_index(drop=True)\n",
        "\n",
        "data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcIaqxf7PVE8"
      },
      "outputs": [],
      "source": [
        "class RocketLeagueDataset(Dataset):\n",
        "  def __init__(self, data, apply_recency=True, decay_rate=0.01):\n",
        "    super().__init__()\n",
        "    self.team_features = torch.LongTensor(data['team_sequences'].tolist())\n",
        "    self.opp_features = torch.LongTensor(data['opp_sequences'].tolist())\n",
        "    self.labels = torch.LongTensor(data['win'])\n",
        "\n",
        "    if apply_recency:\n",
        "      n = self.labels.size(0)\n",
        "      indices = np.arange(n)\n",
        "      weights = np.exp(decay_rate * indices)\n",
        "\n",
        "      self.weights = torch.FloatTensor(weights / weights.mean())\n",
        "    else:\n",
        "      self.weights = torch.ones(len(self.labels))\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.team_features.size(0)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.team_features[idx], self.opp_features[idx], self.labels[idx], self.weights[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO9anf894raC"
      },
      "outputs": [],
      "source": [
        "aliases ={\n",
        "    'aboturki': ['abo turki'],\n",
        "    'acronik' : ['acronik 0', 'acro'],\n",
        "    'aqua' : ['ahyqua'],\n",
        "    'ajg' : ['aje','ajf'],\n",
        "    'applesous' : ['apples' , 'appelsous'],\n",
        "    'arafar' : ['ara'],\n",
        "    'archie' : ['arch'],\n",
        "    'arju' : ['ario'],\n",
        "    'atomik' : ['ltk_atomik'],\n",
        "    'atow' : ['atow rikow'],\n",
        "    'aztromick' : ['aztr'],\n",
        "    'badnezz' : ['badnezzrl','badnezzski7'],\n",
        "    'bradk1ng' : ['brad', 'brad la chef', 'bradquentaog7s', 'bradsonmalado', 'lebrad', 'k1ng', 'prime brad'],\n",
        "    'breezi' : ['breezy'],\n",
        "    'brenox' : ['brenox3k'],\n",
        "    'cheese' : ['cheee'],\n",
        "    'colonel' : ['colonel is lagging'],\n",
        "    'crr' : ['crn', 'crrdd'],\n",
        "    'davitrox' : ['davi'],\n",
        "    'droppz' : ['droppzk3k'],\n",
        "    'eekso' : ['eeko', 'eeksoszn'],\n",
        "    'eliakim' : ['eliakimza'],\n",
        "    'ellil' : ['ell1srai', 'elil'],\n",
        "    'evoh' : ['evohpaniniwine', 'oevoh'],\n",
        "    'exfusion_' : ['exfusionz_2'],\n",
        "    'fiberr' : ['pwr fiberr'],\n",
        "    'giuk' : ['giuk_rl'],\n",
        "    'gus' : ['gustexioz'],\n",
        "    'hyderr' : ['hyderr new binds'],\n",
        "    'insp1re' : ['inspire'],\n",
        "    'israkan' : ['israkan 515'],\n",
        "    'ivan' : ['ivn'],\n",
        "    'joyo' : ['52 300lbs chud'],\n",
        "    'jweyts' : ['jweytski7'],\n",
        "    'kevinacho' : ['kevin', 'kevinacho ama pl'],\n",
        "    'kaka' : ['kaka0mame'],\n",
        "    'klaus' : ['klaus queridinho', 'klausrl1'],\n",
        "    'kofyr' : ['koflii'],\n",
        "    'kv1' : ['kv1exe'],\n",
        "    'leodknn' : ['leodkn1'],\n",
        "    'life' : ['life made day 4'],\n",
        "    'lucas06' : ['luca06'],\n",
        "    'lxucha' : ['lxucha lpwr'],\n",
        "    'lynx' : ['lyn', 'lynx555'],\n",
        "    'm7md' : ['m7md97', 'm7md97_rl'],\n",
        "    'majicbear' : ['majibear'],\n",
        "    'mass' : ['massrl'],\n",
        "    'mech' : ['mech streamin', 'mechnew setupunrusting', 'nomech'],\n",
        "    'misery' : ['misery lc'],\n",
        "    'mtzr' : ['mtzrito', 'mtzr_'],\n",
        "    'nmj' : ['njm515'],\n",
        "    'noahsaki' : ['19noahsaki'],\n",
        "    'nym' : ['nym0'],\n",
        "    'plgabriel' : ['plgabriel ama reis'],\n",
        "    'radosin' : ['radosin75', 'radosinho'],\n",
        "    'reis' : ['reis ama kevinacho', 'reis zoo', 'reisss'],\n",
        "    'retals' : ['retal'],\n",
        "    'reysbull' : ['reysmalado', 'reysss'],\n",
        "    'roods' : ['roodsrl'],\n",
        "    'rizon' : ['rizun'],\n",
        "    'rts' : ['rtsz'],\n",
        "    'saizen' : ['saizen rl', '17saizen'],\n",
        "    'simas' : ['12simas'],\n",
        "    'sweaty' : ['sweaty_clarence','sweaty_mclarence'],\n",
        "    'scrzbbles' : ['scrib', 'scribkep', 'scrib buyssgbundle'],\n",
        "    'stain' : ['stainy'],\n",
        "    'teschow' : ['teschow rikow'],\n",
        "    'twnzr' : ['twnzr13'],\n",
        "    'valid' : ['valid broken ulna', 'validvalidvalidvvalidvalidvalid', 'gz validraxxus', 'gz validzetsubusoro'],\n",
        "    'viitin': ['viitin new bind'],\n",
        "    'vulty' : ['vultacus_ live'],\n",
        "    'wellace' : ['well1']\n",
        "}\n",
        "\n",
        "filename = '/content/drive/MyDrive/Public RL/Complete ELO Model/aliases.pkl'\n",
        "with open(filename, 'wb') as f:\n",
        "  pickle.dump(aliases, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQFBoU3oNdM8"
      },
      "outputs": [],
      "source": [
        "#receives: list of player names\n",
        "#outputs: cleaned list of names\n",
        "def clean_names_list(names_list):\n",
        "  for i in range(len(names_list)):\n",
        "    names_list[i] = re.sub(r'[^\\w\\s]','',names_list[i].lower()).strip()\n",
        "  return names_list\n",
        "\n",
        "#receives: list of cleaned player names\n",
        "#outputs: aliased series of team lists\n",
        "def clean_aliases(team_players, aliases):\n",
        "  full_aliases = {word: key for key in aliases.keys() for word in aliases[key]}\n",
        "  return [full_aliases[p] if p in full_aliases else p for p in team_players ]\n",
        "\n",
        "#receives: 2 series of player lists\n",
        "#outputs: vocab dict for embedding\n",
        "def build_vocab(team_players, opp_players, aliases):\n",
        "  vocab = {'<PAD>':0}\n",
        "  all_tokens = []\n",
        "  team_players = team_players.apply(clean_names_list)\n",
        "  opp_players = opp_players.apply(clean_names_list)\n",
        "  team_players = team_players.apply(clean_aliases, args=(aliases,))\n",
        "  opp_players = opp_players.apply(clean_aliases, args=(aliases,))\n",
        "  for team in team_players:\n",
        "    all_tokens.extend(team)\n",
        "  for opp in opp_players:\n",
        "    all_tokens.extend(opp)\n",
        "  player_counts = Counter(all_tokens)\n",
        "  for idx, (player, _) in enumerate(player_counts.most_common(), start=1):\n",
        "    vocab[player] = idx\n",
        "  return vocab\n",
        "\n",
        "#testing method\n",
        "#receives: series of player lists\n",
        "#outputs: list of unique player names\n",
        "def check_names(team_players, opp_players):\n",
        "  all_names = []\n",
        "  for team in team_players:\n",
        "    all_names.extend(team)\n",
        "  for opp in opp_players:\n",
        "    all_names.extend(opp)\n",
        "  return sorted(set(all_names))\n",
        "\n",
        "#receives: list of player names, embedding vocab dict\n",
        "#outputs: player name sequence\n",
        "def team_to_sequence(team_players, vocab, aliases):\n",
        "  return [vocab.get(p, 0) for p in clean_aliases(clean_names_list(team_players), aliases)]\n",
        "\n",
        "#receives: sequence of player name indices\n",
        "#outputs: padded sequence\n",
        "def pad_sequence(sequence, max_len):\n",
        "  if len(sequence) >= max_len:\n",
        "    return sequence[:max_len]\n",
        "  else:\n",
        "    return sequence + [0] * (max_len - len(sequence))\n",
        "\n",
        "#receives: 2 series of player name lists, vocab dict, max sequence length int\n",
        "#outputs: 2 series of padded team sequences\n",
        "def convert_and_pad_sequences(team_players, opp_players, vocab, aliases, max_len):\n",
        "  team_sequences = [team_to_sequence(team, vocab, aliases) for team in team_players]\n",
        "  opp_sequences = [team_to_sequence(opp, vocab, aliases) for opp in opp_players]\n",
        "  padded_team_sequences = [pad_sequence(seq, max_len) for seq in team_sequences]\n",
        "  padded_opp_sequences = [pad_sequence(seq, max_len) for seq in opp_sequences]\n",
        "  return padded_team_sequences, padded_opp_sequences\n",
        "\n",
        "\n",
        "#receives: 2 series of player name lists, max_sequence length int\n",
        "#outputs: 2 series of padded player name sequences, vocab dict\n",
        "def preprocessing_pipeline(team_players, opp_players, aliases, max_len):\n",
        "  vocab = build_vocab(team_players, opp_players, aliases)\n",
        "  team_sequences, opp_sequences = convert_and_pad_sequences(team_players, opp_players, vocab, aliases, max_len)\n",
        "  return team_sequences, opp_sequences, vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmK41jZfPHwJ"
      },
      "outputs": [],
      "source": [
        "def add_reverse_fixtures(frame):\n",
        "  reverse_frame = frame.copy()\n",
        "  reverse_frame = reverse_frame[['opp_sequences', 'team_sequences', 'win']]\n",
        "  reverse_frame['win'] = 1 - reverse_frame['win']\n",
        "  reverse_frame.columns = ['team_sequences', 'opp_sequences', 'win']\n",
        "\n",
        "  frame = frame.reset_index()\n",
        "  reverse_frame = reverse_frame.reset_index()\n",
        "\n",
        "  frame = frame.rename(columns={'index': 'order'})\n",
        "  reverse_frame = reverse_frame.rename(columns={'index': 'order'})\n",
        "  frame['order'] = frame['order'] * 2\n",
        "  reverse_frame['order'] = reverse_frame['order'] * 2 + 1\n",
        "\n",
        "  combined = pd.concat([frame, reverse_frame], ignore_index=True)\n",
        "  combined = combined.sort_values(by='order', ascending=True).drop('order', axis=1).reset_index(drop=True)\n",
        "\n",
        "  return combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_UAAsCYIlP-"
      },
      "outputs": [],
      "source": [
        "#separate train, test, and valid data\n",
        "max_len = 3\n",
        "\n",
        "model_data = data.copy()\n",
        "model_data['team_sequences'], model_data['opp_sequences'], vocab = preprocessing_pipeline(model_data['team_players'], model_data['opp_players'], aliases, max_len)\n",
        "model_data = model_data[['team_sequences','opp_sequences', 'win']]\n",
        "\n",
        "#prepare training and test sets\n",
        "\n",
        "train, test = model_data.loc[:round(model_data.shape[0]*0.9)], model_data[round(model_data.shape[0]*0.9)+1:]\n",
        "train, valid = train_test_split(train, test_size=0.2, random_state=42)\n",
        "train = train.sort_index()\n",
        "valid = valid.sort_index()\n",
        "train = train.reset_index(drop=True)\n",
        "valid = valid.reset_index(drop=True)\n",
        "\n",
        "train = add_reverse_fixtures(train)\n",
        "valid = add_reverse_fixtures(valid)\n",
        "\n",
        "test = test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0ZFlQXTSHb9"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "class to implement early stopping\n",
        "\n",
        "takes: patience (int), delta (float), verbose (bool)\n",
        "\n",
        "methods: check_early_stop(val_loss) - outputs boolean to stop training\n",
        "'''\n",
        "class EarlyStopping():\n",
        "    def __init__(self, patience=5, delta=0, verbose=False):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.verbose = verbose\n",
        "        self.best_loss = None\n",
        "        self.no_improvement_count = 0\n",
        "        self.stop_training = False\n",
        "    def check_early_stop(self, val_loss):\n",
        "        if self.best_loss is None or val_loss< self.best_loss-self.delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.no_improvement_count = 0\n",
        "        else:\n",
        "            self.no_improvement_count += 1\n",
        "            if self.no_improvement_count >= self.patience:\n",
        "                self.stop_training = True\n",
        "                if self.verbose:\n",
        "                    print('Stopping early due to no/negligible improvment')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6stVPkzm3jj2"
      },
      "outputs": [],
      "source": [
        "class PoolPredictionNet(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim=64, fc_hidden_dim1=256, fc_hidden_dim2=64, dropout1=0.3, dropout2=0.3, num_heads=4):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(\n",
        "        vocab_size,\n",
        "        embed_dim\n",
        "    )\n",
        "    self.attention = nn.MultiheadAttention(embed_dim, num_heads=num_heads, batch_first=True)\n",
        "    self.fc1 = nn.Linear(embed_dim*6, fc_hidden_dim1)\n",
        "    self.bn1 = nn.BatchNorm1d(fc_hidden_dim1)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.dropout1 = nn.Dropout(dropout1)\n",
        "    self.fc2 = nn.Linear(fc_hidden_dim1, fc_hidden_dim2)\n",
        "    self.bn2 = nn.BatchNorm1d(fc_hidden_dim2)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.dropout2 = nn.Dropout(dropout2)\n",
        "    self.fc3 = nn.Linear(fc_hidden_dim2, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
        "    init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
        "    init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "  def forward(self, team_features, opp_features):\n",
        "    team_embeddings = self.embedding(team_features)\n",
        "    opp_embeddings = self.embedding(opp_features)\n",
        "\n",
        "    team_attended, _ = self.attention(team_embeddings, team_embeddings, team_embeddings)\n",
        "    opp_attended, _ = self.attention(opp_embeddings, opp_embeddings, opp_embeddings)\n",
        "\n",
        "    team_mean = team_attended.mean(dim=1)        # Average skill/style\n",
        "    team_max = team_attended.max(dim=1)[0]       # Star player effect\n",
        "    team_min = team_attended.min(dim=1)[0]       # Weakest link\n",
        "\n",
        "    opp_mean = opp_attended.mean(dim=1)\n",
        "    opp_max = opp_attended.max(dim=1)[0]\n",
        "    opp_min = opp_attended.min(dim=1)[0]\n",
        "\n",
        "\n",
        "    x = torch.cat([team_mean, team_max, team_min, opp_mean, opp_max, opp_min], dim=1)\n",
        "\n",
        "    x= self.relu1(self.bn1(self.fc1(x)))\n",
        "    x = self.dropout1(x)\n",
        "    x = self.relu2(self.bn2(self.fc2(x)))\n",
        "    x = self.dropout2(x)\n",
        "    x = self.sigmoid(self.fc3(x))\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IneTDuDypkBS"
      },
      "outputs": [],
      "source": [
        "def train_and_validate(model, train, valid, lr, batch_size, weight_decay=1e-5, recency_decay = 0.01, epochs=100, patience=5, shuffle=True):\n",
        "  #instantiate dataloaders\n",
        "  dataset_train = RocketLeagueDataset(train, apply_recency=True, decay_rate=recency_decay)\n",
        "  dataset_valid = RocketLeagueDataset(valid, apply_recency=True, decay_rate=recency_decay)\n",
        "\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=shuffle)\n",
        "  dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  criterion = nn.BCELoss(reduction='none')\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "  early_stopping = EarlyStopping(patience=patience)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    #training section\n",
        "    train_loss = 0\n",
        "    model.train()\n",
        "    for team_features, opp_features, labels, weights in dataloader_train:\n",
        "      model.zero_grad()\n",
        "      outputs = model(team_features, opp_features)\n",
        "\n",
        "      loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "      weighted_loss = (loss * weights.unsqueeze(1)).mean()\n",
        "\n",
        "\n",
        "      weighted_loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += weighted_loss.item()\n",
        "    train_loss /= len(dataloader_train)\n",
        "\n",
        "    #validation section\n",
        "    val_loss = 0\n",
        "    model.eval()\n",
        "    for team_features, opp_features, labels, _ in dataloader_valid:\n",
        "      with torch.no_grad():\n",
        "        outputs = model(team_features, opp_features)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1).float()).mean()\n",
        "        val_loss += loss.item()\n",
        "    val_loss /= len(dataloader_valid)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    early_stopping.check_early_stop(val_loss)\n",
        "    if early_stopping.stop_training:\n",
        "      print(f'Early stopping at epoch {epoch}')\n",
        "      return val_loss, model\n",
        "  return val_loss, model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSaywrn7tyXT"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "  embed_dim = trial.suggest_categorical('embed_dim', [64, 128])\n",
        "  num_heads = trial.suggest_categorical('num_heads', [2, 4, 8])\n",
        "  fc_hidden_dim1 = trial.suggest_categorical('fc_hidden_dim1', [256])\n",
        "  fc_hidden_dim2 = trial.suggest_categorical('fc_hidden_dim2', [32, 64])\n",
        "  dropout1 = trial.suggest_float('dropout1', 0.1, 0.3, step=0.1)\n",
        "  dropout2 = trial.suggest_float('dropout2', 0.2, 0.5, step=0.1)\n",
        "  lr = trial.suggest_float('lr', 0.0001, 0.01, log=True)\n",
        "  batch_size = trial.suggest_categorical('batch_size', [32])\n",
        "  weight_decay = trial.suggest_float('weight_decay', 0.0003, 0.001, log=True)\n",
        "  recency_decay = trial.suggest_float('recency_decay', 1e-5, 3e-5, log=True)\n",
        "\n",
        "  # Constraint: embed_dim must be divisible by num_heads\n",
        "  if embed_dim % num_heads != 0:\n",
        "    raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "  model = PoolPredictionNet(\n",
        "      vocab_size=len(vocab),\n",
        "      embed_dim=embed_dim,\n",
        "      num_heads=num_heads,\n",
        "      fc_hidden_dim1=fc_hidden_dim1,\n",
        "      fc_hidden_dim2=fc_hidden_dim2,\n",
        "      dropout1=dropout1,\n",
        "      dropout2=dropout2,\n",
        "  )\n",
        "\n",
        "  val_loss, _ = train_and_validate(model, train, valid, lr, batch_size, weight_decay, recency_decay)\n",
        "  return val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5BBOQzYve_O",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# study = optuna.create_study(direction='minimize')\n",
        "# study.optimize(objective, n_trials=50)\n",
        "\n",
        "# print('Best hyperparameters', study.best_params)\n",
        "# print('Best validation loss', study.best_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c60V5UysDFjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b185e8-c642-4a05-c438-aadd5b1c4a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 12\n",
            "Test Accuracy: 0.6528149843215942\n"
          ]
        }
      ],
      "source": [
        "model = PoolPredictionNet(\n",
        "    vocab_size=len(vocab),\n",
        "    embed_dim=64,\n",
        "    fc_hidden_dim1=256,\n",
        "    fc_hidden_dim2=64,\n",
        "    dropout1=0.3,\n",
        "    dropout2=0.3,\n",
        "\n",
        ")\n",
        "_, model = train_and_validate(model, train, valid, lr=0.0020102639361487444, batch_size=32, weight_decay=0.0009504475883440148, recency_decay=1.2616293408204758e-05)\n",
        "\n",
        "dataset_test = RocketLeagueDataset(test, apply_recency=False)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
        "\n",
        "acc = Accuracy(task='binary')\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for team_features, opp_features, labels, _ in dataloader_test:\n",
        "    test_probs = model(team_features, opp_features)\n",
        "    test_preds = (test_probs > 0.5)\n",
        "    acc(test_preds, labels.unsqueeze(1))\n",
        "accuracy = acc.compute()\n",
        "print(f'Test Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQrf9QbK42UR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60cfc1c6-f592-4a0d-b58e-c57f49ff55eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 15\n"
          ]
        }
      ],
      "source": [
        "train_final, valid_final = train_test_split(model_data, test_size=0.2, random_state=42)\n",
        "train_final = train_final.sort_index()\n",
        "valid_final = valid_final.sort_index()\n",
        "train_final = train_final.reset_index(drop=True)\n",
        "valid_final = valid_final.reset_index(drop=True)\n",
        "\n",
        "train_final = add_reverse_fixtures(train_final)\n",
        "valid_final = add_reverse_fixtures(valid_final)\n",
        "\n",
        "model = PoolPredictionNet(\n",
        "    vocab_size=len(vocab),\n",
        "    embed_dim=64,\n",
        "    fc_hidden_dim1=256,\n",
        "    fc_hidden_dim2=64,\n",
        "    dropout1=0.3,\n",
        "    dropout2=0.3,\n",
        "\n",
        ")\n",
        "\n",
        "_, model = train_and_validate(model, train_final, valid_final, lr=0.010327914968888427, batch_size=64, weight_decay=0.00169633048876229329, recency_decay=1.0042838177194864e-06)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Public RL/Complete ELO Model/model_state_dict.pth'\n",
        "torch.save(model.state_dict(), path)\n",
        "\n",
        "filename = '/content/drive/MyDrive/Public RL/Complete ELO Model/vocab_dict.pkl'\n",
        "with open(filename, \"wb\") as file:\n",
        "    pickle.dump(vocab, file)"
      ],
      "metadata": {
        "id": "J5mkYUkWiGoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr><hr><br>"
      ],
      "metadata": {
        "id": "J6se-_84Ixp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction Section\n",
        "<p>Below is the code necessary to make predictions. Run each segment until the next instructions block.</p>\n",
        "<br><hr><hr>"
      ],
      "metadata": {
        "id": "U3HF1Lk8pqgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torchmetrics\n",
        "%pip install optuna\n",
        "%pip install gdown\n",
        "\n",
        "import time\n",
        "import requests\n",
        "import re\n",
        "import random\n",
        "import pickle\n",
        "import json\n",
        "import torch\n",
        "import math\n",
        "import random\n",
        "import optuna\n",
        "import gdown\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as f\n",
        "import torch.nn.init as init\n",
        "\n",
        "from collections import deque, Counter\n",
        "from datetime import datetime, timedelta\n",
        "from scipy import stats\n",
        "from torch import nn\n",
        "from torchmetrics import Accuracy\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from pathlib import Path\n",
        "from google.colab import userdata\n",
        "from google.colab import files\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TogHyY7FI6Uv",
        "outputId": "d381bd7f-b502-4b0f-d157-57de7a42b495",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cpu)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.45)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.11.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PoolPredictionNet(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim=64, fc_hidden_dim1=256, fc_hidden_dim2=64, dropout1=0.3, dropout2=0.3, num_heads=4):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(\n",
        "        vocab_size,\n",
        "        embed_dim\n",
        "    )\n",
        "    self.attention = nn.MultiheadAttention(embed_dim, num_heads=num_heads, batch_first=True)\n",
        "    self.fc1 = nn.Linear(embed_dim*6, fc_hidden_dim1)\n",
        "    self.bn1 = nn.BatchNorm1d(fc_hidden_dim1)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.dropout1 = nn.Dropout(dropout1)\n",
        "    self.fc2 = nn.Linear(fc_hidden_dim1, fc_hidden_dim2)\n",
        "    self.bn2 = nn.BatchNorm1d(fc_hidden_dim2)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.dropout2 = nn.Dropout(dropout2)\n",
        "    self.fc3 = nn.Linear(fc_hidden_dim2, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
        "    init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
        "    init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "  def forward(self, team_features, opp_features):\n",
        "    team_embeddings = self.embedding(team_features)\n",
        "    opp_embeddings = self.embedding(opp_features)\n",
        "\n",
        "    team_attended, _ = self.attention(team_embeddings, team_embeddings, team_embeddings)\n",
        "    opp_attended, _ = self.attention(opp_embeddings, opp_embeddings, opp_embeddings)\n",
        "\n",
        "    team_mean = team_attended.mean(dim=1)        # Average skill/style\n",
        "    team_max = team_attended.max(dim=1)[0]       # Star player effect\n",
        "    team_min = team_attended.min(dim=1)[0]       # Weakest link\n",
        "\n",
        "    opp_mean = opp_attended.mean(dim=1)\n",
        "    opp_max = opp_attended.max(dim=1)[0]\n",
        "    opp_min = opp_attended.min(dim=1)[0]\n",
        "\n",
        "\n",
        "    x = torch.cat([team_mean, team_max, team_min, opp_mean, opp_max, opp_min], dim=1)\n",
        "\n",
        "    x= self.relu1(self.bn1(self.fc1(x)))\n",
        "    x = self.dropout1(x)\n",
        "    x = self.relu2(self.bn2(self.fc2(x)))\n",
        "    x = self.dropout2(x)\n",
        "    x = self.sigmoid(self.fc3(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "  #receives: list of player names\n",
        "#outputs: cleaned list of names\n",
        "def clean_names_list(names_list):\n",
        "  for i in range(len(names_list)):\n",
        "    names_list[i] = re.sub(r'[^\\w\\s]','',names_list[i].lower()).strip()\n",
        "  return names_list\n",
        "\n",
        "#receives: list of cleaned player names\n",
        "#outputs: aliased series of team lists\n",
        "def clean_aliases(team_players, aliases):\n",
        "  full_aliases = {word: key for key in aliases.keys() for word in aliases[key]}\n",
        "  return [full_aliases[p] if p in full_aliases else p for p in team_players ]\n",
        "\n",
        "#receives: 2 series of player lists\n",
        "#outputs: vocab dict for embedding\n",
        "def build_vocab(team_players, opp_players, aliases):\n",
        "  vocab = {'<PAD>':0}\n",
        "  all_tokens = []\n",
        "  team_players = team_players.apply(clean_names_list)\n",
        "  opp_players = opp_players.apply(clean_names_list)\n",
        "  team_players = team_players.apply(clean_aliases, args=(aliases,))\n",
        "  opp_players = opp_players.apply(clean_aliases, args=(aliases,))\n",
        "  for team in team_players:\n",
        "    all_tokens.extend(team)\n",
        "  for opp in opp_players:\n",
        "    all_tokens.extend(opp)\n",
        "  player_counts = Counter(all_tokens)\n",
        "  for idx, (player, _) in enumerate(player_counts.most_common(), start=1):\n",
        "    vocab[player] = idx\n",
        "  return vocab\n",
        "\n",
        "#testing method\n",
        "#receives: series of player lists\n",
        "#outputs: list of unique player names\n",
        "def check_names(team_players, opp_players):\n",
        "  all_names = []\n",
        "  for team in team_players:\n",
        "    all_names.extend(team)\n",
        "  for opp in opp_players:\n",
        "    all_names.extend(opp)\n",
        "  return sorted(set(all_names))\n",
        "\n",
        "#receives: list of player names, embedding vocab dict\n",
        "#outputs: player name sequence\n",
        "def team_to_sequence(team_players, vocab, aliases):\n",
        "  return [vocab.get(p, 0) for p in clean_aliases(clean_names_list(team_players), aliases)]\n",
        "\n",
        "#receives: sequence of player name indices\n",
        "#outputs: padded sequence\n",
        "def pad_sequence(sequence, max_len):\n",
        "  if len(sequence) >= max_len:\n",
        "    return sequence[:max_len]\n",
        "  else:\n",
        "    return sequence + [0] * (max_len - len(sequence))\n",
        "\n",
        "#receives: 2 series of player name lists, vocab dict, max sequence length int\n",
        "#outputs: 2 series of padded team sequences\n",
        "def convert_and_pad_sequences(team_players, opp_players, vocab, aliases, max_len):\n",
        "  team_sequences = [team_to_sequence(team, vocab, aliases) for team in team_players]\n",
        "  opp_sequences = [team_to_sequence(opp, vocab, aliases) for opp in opp_players]\n",
        "  padded_team_sequences = [pad_sequence(seq, max_len) for seq in team_sequences]\n",
        "  padded_opp_sequences = [pad_sequence(seq, max_len) for seq in opp_sequences]\n",
        "  return padded_team_sequences, padded_opp_sequences\n",
        "\n",
        "\n",
        "#receives: 2 series of player name lists, max_sequence length int\n",
        "#outputs: 2 series of padded player name sequences, vocab dict\n",
        "def preprocessing_pipeline(team_players, opp_players, aliases, max_len):\n",
        "  vocab = build_vocab(team_players, opp_players, aliases)\n",
        "  team_sequences, opp_sequences = convert_and_pad_sequences(team_players, opp_players, vocab, aliases, max_len)\n",
        "  return team_sequences, opp_sequences, vocab\n",
        "\n",
        "\n",
        "\n",
        "class RocketLeagueDataset(Dataset):\n",
        "  def __init__(self, data, apply_recency=True, decay_rate=0.01):\n",
        "    super().__init__()\n",
        "    self.team_features = torch.LongTensor(data['team_sequences'].tolist())\n",
        "    self.opp_features = torch.LongTensor(data['opp_sequences'].tolist())\n",
        "    self.labels = torch.LongTensor(data['win'])\n",
        "\n",
        "    if apply_recency:\n",
        "      n = self.labels.size(0)\n",
        "      indices = np.arange(n)\n",
        "      weights = np.exp(decay_rate * indices)\n",
        "\n",
        "      self.weights = torch.FloatTensor(weights / weights.mean())\n",
        "    else:\n",
        "      self.weights = torch.ones(len(self.labels))\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.team_features.size(0)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.team_features[idx], self.opp_features[idx], self.labels[idx], self.weights[idx]\n"
      ],
      "metadata": {
        "id": "jpsu6DkUwtRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_FILE_ID = \"1zbwMlsNe5MKJsZV4AHTTZ_wZHDMleziI\"\n",
        "VOCAB_FILE_ID = \"1e0ijSq9TVTdfm36bqR8925u7dqw7WeFv\"\n",
        "ALIASES_FILE_ID = \"1ou5x9zX8kU8y_hLFgWNsOmxe7AxxbAjF\"\n",
        "\n",
        "# Download model state dict\n",
        "print(\"Downloading model state dict...\")\n",
        "model_path = \"model_state_dict.pth\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={MODEL_FILE_ID}\", model_path, quiet=False, fuzzy=True)\n",
        "\n",
        "# Download vocab dictionary\n",
        "print(\"Downloading vocab dictionary...\")\n",
        "vocab_path = \"vocab_dict.pkl\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={VOCAB_FILE_ID}\", vocab_path, quiet=False, fuzzy=True)\n",
        "\n",
        "# Download aliases\n",
        "print(\"Downloading aliases...\")\n",
        "aliases_path = \"aliases.pkl\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={ALIASES_FILE_ID}\", aliases_path, quiet=False, fuzzy=True)\n",
        "\n",
        "# Load the files\n",
        "print(\"Loading model state dict...\")\n",
        "model_state_dict = torch.load(model_path, map_location='cpu')\n",
        "\n",
        "print(\"Loading vocab dictionary...\")\n",
        "with open(vocab_path, 'rb') as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "print(\"Loading aliases...\")\n",
        "with open(aliases_path, 'rb') as f:\n",
        "    aliases = pickle.load(f)\n",
        "\n",
        "print(\"Download and loading complete!\")\n",
        "print(f\"Model keys: {list(model_state_dict.keys())[:5]}...\")\n",
        "print(f\"Vocab size: {len(vocab)}\")\n",
        "print(f\"Aliases loaded: {len(aliases) if isinstance(aliases, (dict, list)) else 'N/A'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwG24K-4Jzjo",
        "outputId": "9e7c436e-f175-4619-c1dd-518706d8e654",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model state dict...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zbwMlsNe5MKJsZV4AHTTZ_wZHDMleziI\n",
            "To: /content/model_state_dict.pth\n",
            "100%|██████████| 729k/729k [00:00<00:00, 27.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading vocab dictionary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1e0ijSq9TVTdfm36bqR8925u7dqw7WeFv\n",
            "To: /content/vocab_dict.pkl\n",
            "100%|██████████| 8.77k/8.77k [00:00<00:00, 17.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading aliases...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ou5x9zX8kU8y_hLFgWNsOmxe7AxxbAjF\n",
            "To: /content/aliases.pkl\n",
            "100%|██████████| 2.13k/2.13k [00:00<00:00, 7.51MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model state dict...\n",
            "Loading vocab dictionary...\n",
            "Loading aliases...\n",
            "Download and loading complete!\n",
            "Model keys: ['embedding.weight', 'attention.in_proj_weight', 'attention.in_proj_bias', 'attention.out_proj.weight', 'attention.out_proj.bias']...\n",
            "Vocab size: 739\n",
            "Aliases loaded: 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate model with downloaded structure and vocab\n",
        "\n",
        "model = PoolPredictionNet(len(vocab))\n",
        "model.load_state_dict(model_state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgFOFL_nIP3T",
        "outputId": "72db2019-91b4-499d-d774-f113c56c9079",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSP0aKSdodKv"
      },
      "outputs": [],
      "source": [
        "def simulate_series(pred_team, pred_opp, vocab, aliases, max_len=3, series_length=5, sim_length=1000):\n",
        "  pred_team = pad_sequence(team_to_sequence(pred_team, vocab, aliases), max_len)\n",
        "  pred_opp = pad_sequence(team_to_sequence(pred_opp, vocab, aliases), max_len)\n",
        "\n",
        "  pred_team = torch.LongTensor(pred_team).unsqueeze(0)\n",
        "  pred_opp = torch.LongTensor(pred_opp).unsqueeze(0)\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    team_prob_first = float(model(pred_team, pred_opp).squeeze())\n",
        "    team_prob_second = 1-float(model(pred_opp, pred_team).squeeze())\n",
        "\n",
        "  team_prob = (team_prob_first + team_prob_second) / 2\n",
        "  team_wins, opp_wins = pd.Series(), pd.Series()\n",
        "  for i in range(sim_length):\n",
        "    sim_wins = 0\n",
        "    sim_losses = 0\n",
        "    while ((sim_wins < series_length/2) & (sim_losses < series_length/2)):\n",
        "      if (test := random.random()) < team_prob:\n",
        "        sim_wins += 1\n",
        "      else:\n",
        "        sim_losses += 1\n",
        "    team_wins[i] = sim_wins\n",
        "    opp_wins[i] = sim_losses\n",
        "  team_wins = team_wins.mean()\n",
        "  opp_wins = opp_wins.mean()\n",
        "\n",
        "  if team_wins > opp_wins:\n",
        "    team_wins = math.ceil(series_length/2)\n",
        "    opp_wins = round(opp_wins, 0)\n",
        "  else:\n",
        "    opp_wins = math.ceil(series_length/2)\n",
        "    team_wins = round(team_wins,0)\n",
        "\n",
        "  return [int(team_wins), int(opp_wins)], team_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr><hr><br><p>In the segment below, there is a list of RLCS teams. The team name is on the left, and the list of players is on the right. To simulate a series from these teams, run the cell below and make sure to select teams from the list as they are spelled.</p>\n",
        "<p>To select any 6 players to simulate a hypothetical matchup, use the next cell to enter players individually. Make sure player names are spelled the same way they are in the vocab list that appears if you run the last cell</p><br><hr><hr>"
      ],
      "metadata": {
        "id": "iaSLI-Trp76U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teams_dict = {\n",
        "    'vitality' : ['zen, exotiik','stizzy'],\n",
        "    'kc' : ['vatira', 'atow', 'juicy'],\n",
        "    'nip' : ['joreuz', 'crr', 'oaly'],\n",
        "    'man city' : ['ejby', 'accro', 'tempoh'],\n",
        "    'geekay' : ['apparentlyjack', 'joyo', 'seikoo'],\n",
        "    'novo' : ['nico','acronik', 'giuk'],\n",
        "    'pld' : ['gawfs', 'ethan', 'pluvo'],\n",
        "    'dr' : ['vorce', 'ne0n', 'motion15'],\n",
        "    'cloud' : ['jweyts', 'badnezz', 'wozyen'],\n",
        "    'ght' : ['tms, hyderr', 'gramma'],\n",
        "    'hogan mode' : ['growlii', 'rehzzy', 'rxii'],\n",
        "    'gentlebench' : ['yujin', 'radosin', 'mtzr'],\n",
        "    'tks' : ['thyyder', 'scream', 'kerian'],\n",
        "    'gentle mates' : ['archie', 'nass', 'oski'],\n",
        "    'sonics' : ['toxiic', 'mikeboy', 'smokez'],\n",
        "    'magnifico' : ['atomik', 'tox', 'rezears'],\n",
        "    'ssg' : ['reveal', 'chronic', 'diaz'],\n",
        "    'geng' : ['majicbear', 'justin', 'rise'],\n",
        "    'nrg' : ['atomic', 'daniel', 'beastmode'],\n",
        "    '100x35' : ['crispy', 'simas', 'pzy'],\n",
        "    'lotus' : ['xprt', 'noly', 'wellace'],\n",
        "    'redacted' : ['wahvey', '2piece', 'tawk'],\n",
        "    'sk' : ['arju', 'relatingwave', 'speed'],\n",
        "    'rebellion' : ['firstkiller', 'kofyr', 'lj'],\n",
        "    'gas' : ['garrettg', 'ayyjayy', 'squishy'],\n",
        "    'm80' : ['aris', 'deevo', 'mech'],\n",
        "    'top leh' : ['comm', 'creamz', 'paarth'],\n",
        "    'silence' : ['resonal', 'pigeon', 'lev'],\n",
        "    'unreal nightmare' : ['druee', 'gyro', 'life'],\n",
        "    'feastabonium' : ['s5cosmic', 'ahduhm', 'hazo'],\n",
        "    'fut' : ['cheese', 'frosty', 'sosa'],\n",
        "    'ciel' : ['pndh', 'percy', 'night'],\n",
        "    'pwr' : ['fiberr', 'gus', 'superlachie'],\n",
        "    'wildcard' : ['fever', 'torsos', 'bananahead'],\n",
        "    'furia' : ['yanxnz', 'lostt', 'swiftt'],\n",
        "    'mibr' : ['reysbull', 'aztromick', 'sad'],\n",
        "    'falcons' : ['rw9', 'kiileerrz', 'dralii'],\n",
        "    'twisted minds' : ['nwpo', 'm0nkey m00n', 'trk511'],\n",
        "    'saudi arabia' : ['rw9', 'kiileerrz', 'trk511'],\n",
        "    'oman' : ['abdullah', 'lumber', 'sultan'],\n",
        "    'usa' : ['atomic', 'beastmode', 'daniel'],\n",
        "    'brazil' : ['yanxnz', 'swiftt', 'lostt'],\n",
        "    'chile' : ['reysbull', 'davitrox', 'pan'],\n",
        "    'germany' : ['tox', 'catalysm', 'rezears'],\n",
        "    'italy' : ['arju', 'davoof', 'hyderr'],\n",
        "    'norway' : ['wozyen', 'jup', 'bruhkay'],\n",
        "    'morocco' : ['dralii', 'nass', 'skyrix'],\n",
        "    'australia' : ['fever', 'torsos', 'bananahead'],\n",
        "    'france' : ['zen', 'vatira', 'exotiik'],\n",
        "    'malaysia' : ['sphinx', 'blue', 'misty'],\n",
        "    'england' : ['apparentlyjack', 'archie', 'joyo'],\n",
        "    'belgium' : ['aztral', 'atow', 'rysfox'],\n",
        "    'south africa' : ['2die4', 'gunz', 'snowyy'],\n",
        "    'netherlands' : ['joreuz', 'oaly', 'mikeboy']\n",
        "}"
      ],
      "metadata": {
        "id": "T_4he6BULKQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a\n",
        "a\n",
        "a\n",
        "##simulate from teams\n",
        "team = input('Team 1: ')\n",
        "opp = input('Team 2: ')\n",
        "length = int(input('Series Length (odd only): '))\n",
        "\n",
        "series_score, team_prob = simulate_series(teams_dict[team], teams_dict[opp], vocab, aliases, max_len=3, series_length=length)\n",
        "\n",
        "print(\"\\n\")\n",
        "print('-'*50)\n",
        "print(f'{team.title()} {series_score[0]} - {series_score[1]} {opp.title()}')\n",
        "print(f'{team.title()} Single Game Win Probability: {round(team_prob*100, 2)}%')\n",
        "print('-'*50)"
      ],
      "metadata": {
        "id": "Qc391M40O4fH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b7d5221-d117-48bd-8655-843bfe9a9071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Team 1: fut\n",
            "Team 2: nrg\n",
            "Series Length (odd only): 7\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Fut 2 - 4 Nrg\n",
            "Fut Single Game Win Probability: 36.85%\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#simulate series from players (vocab list below)\n",
        "#if you spell a player name incorrectly, they will be entered as a \"default\" player\n",
        "#default players will affect the prediction differently than the correctly spelled player\n",
        "team1 = input('Team 1, Player 1: ')\n",
        "team2 = input('Team 1, Player 2: ')\n",
        "team3 = input('Team 1, Player 3: ')\n",
        "\n",
        "opp1 = input('Team 2, Player 1: ')\n",
        "opp2 = input('Team 2, Player 2: ')\n",
        "opp3 = input('Team 2, Player 3: ')\n",
        "\n",
        "length = int(input('Series Length (odd only): '))\n",
        "\n",
        "team = [team1, team2, team3]\n",
        "opp = [opp1, opp2, opp3]\n",
        "\n",
        "print('\\n')\n",
        "print(simulate_series(team, opp, vocab, aliases, series_length=length))"
      ],
      "metadata": {
        "id": "3_ysUVtdrc4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62f41fe3-58e1-42fd-c611-a472cccf99ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Team 1, Player 1: a\n",
            "Team 1, Player 2: a\n",
            "Team 1, Player 3: a\n",
            "Team 2, Player 1: a\n",
            "Team 2, Player 2: a\n",
            "Team 2, Player 3: a\n",
            "Series Length (odd only): 7\n",
            "\n",
            "\n",
            "([4, 3], 0.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(list(vocab.keys()))"
      ],
      "metadata": {
        "id": "Tg9JRLvZsZ0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23cab340-55ec-425c-b939-f31da53a8b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '21',\n",
              " '2die4',\n",
              " '2piece',\n",
              " '4ever milo',\n",
              " '7kexii',\n",
              " '7mani',\n",
              " '<PAD>',\n",
              " 'a7md',\n",
              " 'aan1',\n",
              " 'abdullah',\n",
              " 'abo mzoon',\n",
              " 'abood',\n",
              " 'aboturki',\n",
              " 'abscrazy',\n",
              " 'accro',\n",
              " 'acerolaas',\n",
              " 'acronik',\n",
              " 'aemond targaryen',\n",
              " 'aguz',\n",
              " 'ahduhm',\n",
              " 'ajg',\n",
              " 'akai',\n",
              " 'akame',\n",
              " 'akira0902',\n",
              " 'alameri',\n",
              " 'aloneeyy',\n",
              " 'alphinhaa',\n",
              " 'alraz',\n",
              " 'alxx',\n",
              " 'amatel',\n",
              " 'amphis',\n",
              " 'ams',\n",
              " 'andy',\n",
              " 'anyeelo',\n",
              " 'apparentlyjack',\n",
              " 'applesous',\n",
              " 'aqua',\n",
              " 'arafar',\n",
              " 'archie',\n",
              " 'aris',\n",
              " 'arju',\n",
              " 'arrow',\n",
              " 'asn_rubiix',\n",
              " 'atomic',\n",
              " 'atomik',\n",
              " 'atow',\n",
              " 'awareant9767',\n",
              " 'awkwavey',\n",
              " 'axrxs',\n",
              " 'ayman',\n",
              " 'ayyjayy',\n",
              " 'aziz',\n",
              " 'azooz',\n",
              " 'aztral',\n",
              " 'aztromick',\n",
              " 'b',\n",
              " 'b2sel',\n",
              " 'baconhero',\n",
              " 'bad',\n",
              " 'bader',\n",
              " 'badnezz',\n",
              " 'bal',\n",
              " 'balakeplease2 5',\n",
              " 'ballerrees',\n",
              " 'bananacat',\n",
              " 'bananahead',\n",
              " 'bathy',\n",
              " 'baz',\n",
              " 'bazlenks',\n",
              " 'beastmode',\n",
              " 'beasty',\n",
              " 'bebbangboy',\n",
              " 'bemmz',\n",
              " 'ben',\n",
              " 'benji',\n",
              " 'besogoat',\n",
              " 'bfg bossk',\n",
              " 'bfg dreameh',\n",
              " 'bfg milo',\n",
              " 'big gez',\n",
              " 'bigfoot0',\n",
              " 'billy',\n",
              " 'blade',\n",
              " 'blue',\n",
              " 'bluii',\n",
              " 'bnj66',\n",
              " 'bob',\n",
              " 'boitjie tweex',\n",
              " 'bora',\n",
              " 'bosh',\n",
              " 'bradk1ng',\n",
              " 'breezi',\n",
              " 'brenox',\n",
              " 'bruhkay',\n",
              " 'bryza',\n",
              " 'bubble',\n",
              " 'bubz',\n",
              " 'bunney',\n",
              " 'burn',\n",
              " 'caard',\n",
              " 'cabdllah',\n",
              " 'caitg1',\n",
              " 'caleb',\n",
              " 'calzafy',\n",
              " 'captain_flemme',\n",
              " 'carca',\n",
              " 'cata',\n",
              " 'catalysm',\n",
              " 'cb',\n",
              " 'chancla',\n",
              " 'cheese',\n",
              " 'cheetah',\n",
              " 'cherry',\n",
              " 'chicago',\n",
              " 'chikage',\n",
              " 'chippy',\n",
              " 'chronic',\n",
              " 'chub',\n",
              " 'closestork5921',\n",
              " 'cobbo',\n",
              " 'cocoo',\n",
              " 'colonel',\n",
              " 'cometten',\n",
              " 'comm',\n",
              " 'compact',\n",
              " 'cramox',\n",
              " 'crazy',\n",
              " 'creamyy',\n",
              " 'creamz',\n",
              " 'creedeny',\n",
              " 'creepy',\n",
              " 'crespt',\n",
              " 'crispy',\n",
              " 'crr',\n",
              " 'cyx511',\n",
              " 'd',\n",
              " 'daltonza9644',\n",
              " 'daniel',\n",
              " 'dansku_',\n",
              " 'dapz',\n",
              " 'dark',\n",
              " 'darklight',\n",
              " 'darxtz',\n",
              " 'davitrox',\n",
              " 'davoof',\n",
              " 'dayys',\n",
              " 'dcln',\n",
              " 'dead',\n",
              " 'deal',\n",
              " 'declan0',\n",
              " 'deto',\n",
              " 'devour',\n",
              " 'diamo',\n",
              " 'diaz',\n",
              " 'dorito',\n",
              " 'dpplutox',\n",
              " 'dralii',\n",
              " 'dreankz',\n",
              " 'drknown',\n",
              " 'droppz',\n",
              " 'druee',\n",
              " 'drufinho',\n",
              " 'duggy',\n",
              " 'dvo',\n",
              " 'echo',\n",
              " 'eco',\n",
              " 'eden hazard',\n",
              " 'eekso',\n",
              " 'egles',\n",
              " 'ejby',\n",
              " 'eliakim',\n",
              " 'ellil',\n",
              " 'emiilvald',\n",
              " 'energizer',\n",
              " 'enzo',\n",
              " 'erv',\n",
              " 'ethan',\n",
              " 'euxsy',\n",
              " 'evas a tahw',\n",
              " 'ever',\n",
              " 'evoh',\n",
              " 'exfusion_',\n",
              " 'exotiik',\n",
              " 'extra',\n",
              " 'ezgles',\n",
              " 'f',\n",
              " 'fades',\n",
              " 'fbi',\n",
              " 'fera',\n",
              " 'fever',\n",
              " 'fiasco',\n",
              " 'fiberr',\n",
              " 'final crescendo',\n",
              " 'finn',\n",
              " 'firefoxd',\n",
              " 'firstkiller',\n",
              " 'fisal',\n",
              " 'fiv3up',\n",
              " 'flakey',\n",
              " 'flare',\n",
              " 'flitz',\n",
              " 'fnsi',\n",
              " 'folly',\n",
              " 'force',\n",
              " 'forece',\n",
              " 'freethemandem',\n",
              " 'freez astroz',\n",
              " 'frenchie',\n",
              " 'friction',\n",
              " 'frook',\n",
              " 'frootz e',\n",
              " 'frosty',\n",
              " 'fsol',\n",
              " 'fzix',\n",
              " 'galaxy',\n",
              " 'gamble',\n",
              " 'garrettg',\n",
              " 'gawfs',\n",
              " 'ghaazi511',\n",
              " 'ghassan',\n",
              " 'ghost',\n",
              " 'ghostyann',\n",
              " 'giuk',\n",
              " 'glasso',\n",
              " 'glowz',\n",
              " 'goldninja',\n",
              " 'gramma',\n",
              " 'growlii',\n",
              " 'guh',\n",
              " 'gunz',\n",
              " 'gus',\n",
              " 'gyro',\n",
              " 'gz risk',\n",
              " 'haberkamper _',\n",
              " 'hadiryas',\n",
              " 'halden',\n",
              " 'halloww',\n",
              " 'happymeal',\n",
              " 'hashir',\n",
              " 'hatem',\n",
              " 'hawk',\n",
              " 'hazo',\n",
              " 'hb',\n",
              " 'heaven',\n",
              " 'hej',\n",
              " 'her voice mail',\n",
              " 'hexic',\n",
              " 'hireo',\n",
              " 'hisoka',\n",
              " 'hntr',\n",
              " 'hocke',\n",
              " 'hockser',\n",
              " 'hughesy',\n",
              " 'husam',\n",
              " 'hxff',\n",
              " 'hxll0',\n",
              " 'hyderr',\n",
              " 'hypersonic',\n",
              " 'ianpinheiro',\n",
              " 'inav',\n",
              " 'insp1re',\n",
              " 'inuxty',\n",
              " 'irietsz',\n",
              " 'isl6nii',\n",
              " 'islam dunk',\n",
              " 'isma',\n",
              " 'israkan',\n",
              " 'itachi',\n",
              " 'ivan',\n",
              " 'ix',\n",
              " 'jackay',\n",
              " 'jair',\n",
              " 'jarkuu',\n",
              " 'jazii',\n",
              " 'jb0x',\n",
              " 'jeff',\n",
              " 'jelly',\n",
              " 'jimmo',\n",
              " 'jknaps',\n",
              " 'jocse',\n",
              " 'john666',\n",
              " 'jordan',\n",
              " 'joreuz',\n",
              " 'josema',\n",
              " 'joyo',\n",
              " 'jsm',\n",
              " 'juck',\n",
              " 'juicy',\n",
              " 'jup',\n",
              " 'jushu',\n",
              " 'justin',\n",
              " 'justuszzz',\n",
              " 'jweyts',\n",
              " 'k',\n",
              " 'k a i z e n',\n",
              " 'kaito',\n",
              " 'kaka',\n",
              " 'kal',\n",
              " 'kam',\n",
              " 'kash',\n",
              " 'kata rikow',\n",
              " 'kaydop',\n",
              " 'kejay',\n",
              " 'kekkles',\n",
              " 'ken515',\n",
              " 'kepjin',\n",
              " 'kepzen',\n",
              " 'kerian',\n",
              " 'kevinacho',\n",
              " 'khakha0',\n",
              " 'khiernan',\n",
              " 'kiileerrz',\n",
              " 'kisai',\n",
              " 'kito',\n",
              " 'kj',\n",
              " 'klaus',\n",
              " 'kma',\n",
              " 'kns',\n",
              " 'kofyr',\n",
              " 'korp',\n",
              " 'krepezy',\n",
              " 'kroado',\n",
              " 'kryy',\n",
              " 'kuipier',\n",
              " 'kuro',\n",
              " 'kv1',\n",
              " 'kxrma',\n",
              " 'kxrvin',\n",
              " 'kylo',\n",
              " 'l4xinnnn',\n",
              " 'lacks  ar',\n",
              " 'lagly',\n",
              " 'lasa',\n",
              " 'laxeen',\n",
              " 'laxin',\n",
              " 'lazybear',\n",
              " 'lba',\n",
              " 'lbp',\n",
              " 'lct',\n",
              " 'le0',\n",
              " 'leodknn',\n",
              " 'leoro',\n",
              " 'lev',\n",
              " 'lickse',\n",
              " 'life',\n",
              " 'lionblaze',\n",
              " 'little czudo',\n",
              " 'little motion',\n",
              " 'little sweezy',\n",
              " 'lj',\n",
              " 'lord of abha',\n",
              " 'lostt',\n",
              " 'loyal',\n",
              " 'lucas06',\n",
              " 'luiisp',\n",
              " 'lumberj4ck',\n",
              " 'lunar',\n",
              " 'lunarbrawler',\n",
              " 'lunatic',\n",
              " 'lunr',\n",
              " 'luxyy911',\n",
              " 'lxucha',\n",
              " 'lynx',\n",
              " 'm',\n",
              " 'm0nkey m00n',\n",
              " 'm4ykw',\n",
              " 'm6r',\n",
              " 'm7ad',\n",
              " 'm7md',\n",
              " 'm7sn',\n",
              " 'm80 mech',\n",
              " 'madssssss',\n",
              " 'mage',\n",
              " 'majicbear',\n",
              " 'majid',\n",
              " 'mala7',\n",
              " 'manman4124',\n",
              " 'marc_by_8',\n",
              " 'marky',\n",
              " 'mass',\n",
              " 'massrll',\n",
              " 'mat',\n",
              " 'matro',\n",
              " 'matt',\n",
              " 'matth',\n",
              " 'matthew',\n",
              " 'mav',\n",
              " 'max verstappen',\n",
              " 'maxeew',\n",
              " 'mazzen509',\n",
              " 'md',\n",
              " 'me standing in the corner watchi',\n",
              " 'mech',\n",
              " 'meesh18',\n",
              " 'mesho',\n",
              " 'michi',\n",
              " 'mike wazza',\n",
              " 'mikeboy',\n",
              " 'mildpork',\n",
              " 'mimosa 2000',\n",
              " 'mira',\n",
              " 'misery',\n",
              " 'misty',\n",
              " 'mittaen',\n",
              " 'mock',\n",
              " 'mohammed',\n",
              " 'mohammed2',\n",
              " 'mohanned',\n",
              " 'morgenshtern',\n",
              " 'morse',\n",
              " 'motta',\n",
              " 'mouse',\n",
              " 'mozzarella old acc',\n",
              " 'mtzr',\n",
              " 'mu',\n",
              " 'mu3',\n",
              " 'muss',\n",
              " 'mzkn',\n",
              " 'mzxy',\n",
              " 'n a w 2 f',\n",
              " 'n1tr',\n",
              " 'nachosky',\n",
              " 'nado',\n",
              " 'nadr',\n",
              " 'naizak',\n",
              " 'nass',\n",
              " 'nawaf',\n",
              " 'nbvx',\n",
              " 'ne0n',\n",
              " 'neiko',\n",
              " 'nemr',\n",
              " 'neptune',\n",
              " 'nerve',\n",
              " 'nico',\n",
              " 'nitrous',\n",
              " 'nmj',\n",
              " 'nmj515',\n",
              " 'no one',\n",
              " 'no1tbateglazer',\n",
              " 'noahsaki',\n",
              " 'noharaa',\n",
              " 'noly',\n",
              " 'nook',\n",
              " 'nory',\n",
              " 'ntri',\n",
              " 'nush',\n",
              " 'nwpo',\n",
              " 'nx  bang',\n",
              " 'nxghtt',\n",
              " 'nym',\n",
              " 'o0vvl',\n",
              " 'oaly',\n",
              " 'oath',\n",
              " 'obscurity',\n",
              " 'onizumaa',\n",
              " 'ops',\n",
              " 'oscillon',\n",
              " 'oski',\n",
              " 'ovampierz',\n",
              " 'owner of the rift',\n",
              " 'paarth',\n",
              " 'pagsi',\n",
              " 'pan',\n",
              " 'panzer',\n",
              " 'paraapelka',\n",
              " 'peanutss',\n",
              " 'percy',\n",
              " 'pigeon',\n",
              " 'pingu',\n",
              " 'pisky',\n",
              " 'pj',\n",
              " 'plasma',\n",
              " 'player1',\n",
              " 'player2',\n",
              " 'player3',\n",
              " 'plexi',\n",
              " 'plgabriel',\n",
              " 'pluvo',\n",
              " 'pndh',\n",
              " 'porsas52',\n",
              " 'price',\n",
              " 'prodz',\n",
              " 'profile',\n",
              " 'pwr gus',\n",
              " 'pwr superlachie',\n",
              " 'pzy',\n",
              " 'qtipz',\n",
              " 'radosin',\n",
              " 'raed',\n",
              " 'rak',\n",
              " 'ramzy',\n",
              " 'ravilia',\n",
              " 'ravioli',\n",
              " 'ray',\n",
              " 'rayagoat',\n",
              " 'realize',\n",
              " 'rebmob_',\n",
              " 'reekkan',\n",
              " 'regser',\n",
              " 'rehzzy',\n",
              " 'reis',\n",
              " 'relatingwave',\n",
              " 'rellec',\n",
              " 'remiik',\n",
              " 'ren',\n",
              " 'renshiro',\n",
              " 'renzo',\n",
              " 'repeat',\n",
              " 'resh24',\n",
              " 'resonal',\n",
              " 'retals',\n",
              " 'reveal',\n",
              " 'revezy',\n",
              " 'rex',\n",
              " 'rexo',\n",
              " 'reysbull',\n",
              " 'rez',\n",
              " 'rezears',\n",
              " 'riet',\n",
              " 'rigby cat',\n",
              " 'rigorous',\n",
              " 'rise',\n",
              " 'risk',\n",
              " 'ritsune',\n",
              " 'rixy',\n",
              " 'riyal',\n",
              " 'rizex',\n",
              " 'rizon',\n",
              " 'rmn',\n",
              " 'roblah',\n",
              " 'roman',\n",
              " 'roods',\n",
              " 'rooster',\n",
              " 'royales',\n",
              " 'rqfts',\n",
              " 'rrei',\n",
              " 'rts',\n",
              " 'rust ez95',\n",
              " 'rw9',\n",
              " 'rxii',\n",
              " 'rysfox',\n",
              " 'ryuzaki',\n",
              " 'ryzen',\n",
              " 's',\n",
              " 's5cosmic',\n",
              " 'sad',\n",
              " 'saizen',\n",
              " 'saizenrl',\n",
              " 'sam',\n",
              " 'san7tn',\n",
              " 'sato',\n",
              " 'schalkg',\n",
              " 'scorch',\n",
              " 'scream33',\n",
              " 'scrub',\n",
              " 'scrzbbles',\n",
              " 'seck',\n",
              " 'seikoo',\n",
              " 'sek',\n",
              " 'sergeant',\n",
              " 'sevenpack',\n",
              " 'sfireh',\n",
              " 'shad',\n",
              " 'shadow',\n",
              " 'shallow _0',\n",
              " 'shezz',\n",
              " 'shin',\n",
              " 'shisuixnz',\n",
              " 'shoogies',\n",
              " 'shu',\n",
              " 'shugs',\n",
              " 'simas',\n",
              " 'siqii',\n",
              " 'skillsteal',\n",
              " 'skippy',\n",
              " 'skyrix',\n",
              " 'smashy',\n",
              " 'smokez',\n",
              " 'smw',\n",
              " 'snaz',\n",
              " 'snipjz',\n",
              " 'snixy',\n",
              " 'snoozy',\n",
              " 'snowadore',\n",
              " 'snowyy',\n",
              " 'sobral',\n",
              " 'solanun',\n",
              " 'somedieyoung',\n",
              " 'sonr',\n",
              " 'sosa',\n",
              " 'soulgenie',\n",
              " 'soulja4life_7',\n",
              " 'sour',\n",
              " 'sozya1',\n",
              " 'speed',\n",
              " 'sphinx',\n",
              " 'sq',\n",
              " 'sqqp',\n",
              " 'squig',\n",
              " 'squishy',\n",
              " 'staar',\n",
              " 'stain',\n",
              " 'stake',\n",
              " 'starwindss',\n",
              " 'stizzy',\n",
              " 'sultan',\n",
              " 'sum',\n",
              " 'superlachie',\n",
              " 'suprass',\n",
              " 'suspect',\n",
              " 'svrnd',\n",
              " 'swazz',\n",
              " 'sweaty',\n",
              " 'swiftt',\n",
              " 'sxm',\n",
              " 'syke',\n",
              " 'syn',\n",
              " 'sythe',\n",
              " 't3chnic',\n",
              " 't7lm',\n",
              " 't9berh',\n",
              " 'taha273',\n",
              " 'tank',\n",
              " 'taroco',\n",
              " 'tatagane',\n",
              " 'tawk',\n",
              " 'tcorrell',\n",
              " 'tehqoz',\n",
              " 'tempoh',\n",
              " 'tepra',\n",
              " 'terms and conditions',\n",
              " 'teschow',\n",
              " 'the concierge of crime',\n",
              " 'the final boss shadi',\n",
              " 'tho',\n",
              " 'thooberr',\n",
              " 'thunder',\n",
              " 'thyyder',\n",
              " 'timmy oce',\n",
              " 'tims',\n",
              " 'tjester',\n",
              " 'tkay',\n",
              " 'tms',\n",
              " 'toastie',\n",
              " 'tommysalami2017',\n",
              " 'tonights the night',\n",
              " 'tonio',\n",
              " 'torres8232',\n",
              " 'torsos',\n",
              " 'tossis',\n",
              " 'tox',\n",
              " 'toxiic',\n",
              " 'tricky',\n",
              " 'tristiano swartznaldo',\n",
              " 'trk511',\n",
              " 'trook',\n",
              " 'trstxnz',\n",
              " 'ttvmvrsyyy',\n",
              " 'turk1_100',\n",
              " 'twister',\n",
              " 'twistt',\n",
              " 'twiz',\n",
              " 'twnzr',\n",
              " 'u',\n",
              " 'ultimatehntr97',\n",
              " 'umbroken',\n",
              " 'uncle bill',\n",
              " 'v',\n",
              " 'v3ziz',\n",
              " 'valid',\n",
              " 'vantage',\n",
              " 'vatira',\n",
              " 'vav',\n",
              " 'velocity',\n",
              " 'vensi',\n",
              " 'vesh',\n",
              " 'vfbi',\n",
              " 'viitin',\n",
              " 'villain arc',\n",
              " 'vis',\n",
              " 'vksailen',\n",
              " 'vops1',\n",
              " 'vorce',\n",
              " 'vulty',\n",
              " 'w a j e h yt',\n",
              " 'waavy_',\n",
              " 'wahvey',\n",
              " 'waseem',\n",
              " 'washed pro max',\n",
              " 'wee_casper',\n",
              " 'weins',\n",
              " 'welcome to jamrock',\n",
              " 'wellace',\n",
              " 'werthy',\n",
              " 'werty',\n",
              " 'wiiilooo',\n",
              " 'willie',\n",
              " 'wisty',\n",
              " 'wizz',\n",
              " 'wl  pj',\n",
              " 'wolftic',\n",
              " 'worstplejer',\n",
              " 'wozyen',\n",
              " 'wthellynohara',\n",
              " 'wylew',\n",
              " 'x',\n",
              " 'xd6a',\n",
              " 'xeelfine',\n",
              " 'xof',\n",
              " 'xp',\n",
              " 'xprt',\n",
              " 'xrz',\n",
              " 'xyro',\n",
              " 'yagmaiii',\n",
              " 'yand0',\n",
              " 'yanxnz',\n",
              " 'yazeed',\n",
              " 'yis',\n",
              " 'ynxy love sosa',\n",
              " 'youngstar',\n",
              " 'youngstar 0',\n",
              " 'yova',\n",
              " 'yujin',\n",
              " 'yukeo',\n",
              " 'z4l',\n",
              " 'zany',\n",
              " 'zayed',\n",
              " 'zeelai',\n",
              " 'zeeyad',\n",
              " 'zen',\n",
              " 'zenulous',\n",
              " 'zeph',\n",
              " 'zeyad1',\n",
              " 'zineel',\n",
              " 'ziyad',\n",
              " 'zombie',\n",
              " 'zombiefreak',\n",
              " 'ztro']"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}